\chapter{Outlook and future work}
The work presented in this document covered various thematics, all involved in the process of automatically finding new indications for approved drugs. Chapter 1 reviewed the problematic, rationale and drug repositioning efforts that have been done. Chapter 2 argued for a mathematical framework capable of capturing the biomedical knowledge: I presented how description logics (DLs) can be used to achieve this task. The formalisation of the molecular machinery was implemented in Chapter 3 via the Functional Therapeutic Chemical Classification System (FTC). The resource is built by integrating information from various repositories using DLs and reasoning services. As a special care was put on scalability, the resource was successfully implemented in practice. Its content and relevance was evaluated against an existing resource, the Anatomical Therapeutic Chemical Classification System (ATC). Chapter 4 describes how the FTC can be used to perform multiple tasks, such as systematically comparing approved drugs based on their function (similarity-based approach) or to identify potential drug from a known MoA (toolbox approach). The drug repositioning hypotheses generated cover a wide range of therapeutic areas and are open for the community to investigate via a web application. With this last chapter, I wanted to communicate ideas and future projects that can be performed out of the work accomplished during my thesis and presented in this document.

First, the thesis work covers only a section of the overall drug discovery process. I will present in this chapter the next logical steps and the considerations to account for while moving towards laboratory experiments. Secondly and not directly related to drug repositioning, I introduce a follow-up analysis on the way, comparing the definition of function from the FTC perspective against gene expression data. Thirdly and derived from the my personal experience and the theoretical work executed in Chapter 2, I will argue in favour of a simplification of biomedical knowledge representation, based on a straightforward graph structure. Finally, I will briefly describe the clinical relation between off-label use and drug repositioning and how the identification of new indications for a drug can be performed directly from hospital’s data.

\section{Work performed in the context of drug discovery}
Discovering a new active molecule includes an accurate identification of the biological mechanisms behind a disease, followed by the development of chemical compounds screened in assays and further optimised for safety and efficacy. Molecules showing some biological activity in the animal model are then finally tested in clinical trials and approved (//see figure http://blogs.scientificamerican.com/guest-blog/2011/10/06/molecules-to-medicine-clinical-trials-for-beginners/) (//cite 16177777, 12120258). The approach I presented lies in the beginning of this workflow, where the biology and chemistry are characterised, which usually takes 6 years (see Figure XXX) and involve numerous experiments. Assuming I worked a maximum of 3 years on the project, I expect at least 3 more years of experimental work in order to gather enough biological evidence and validate some predictions. As the drugs investigated are all already approved, it should in theory facilitate further development. In practice, drug repositioning can face legal challenges (//ref Chapter 1 section), regarding intellectual protection and safety in particular, therefore it should be assumed that the average timeline (//fig timeline) will be respected.

I am currently looking to set collaborations with research groups performing experiments and bioactivity assays in order to further explore the hypotheses. As many therapeutic areas are covered, priority to one disease type must be defined first. Hereof, anti-cancer agents are a good starting point. There are 84 hypotheses concerning antineoplastic compounds (http://localhost:9000/code/L01), which gives a large enough test case. Phenotypic assays for this class of molecules are well developed and straightforward: given a line of cancer cells exposed to a chemical, it consists at measuring the ratio of cell death against a control (//cite 22460902). The biological context is also better preserved than for target-based assays (//ref chapter 1 section); this format is therefore more suitable to test the hypotheses, as they have been generated while considering their physiological activities in the human body.

In summary, the work done compares and covers the initial exploration of the path towards a new therapy, which continues at this stage by performing the assays to characterise more accurately the predicted pharmacology.

\section{Comparison against functional genomics}
The FTC introduces one possible representation of the function of approved molecules. This descriptor can be used to perform computation on the concept, as illustrated throughout this document. The Connectivity Map (//ref chapter 1 section) defines in another way the biological function of drugs, derived from gene expression experiments. As both methodologies are capturing the concept of function and rely on similarity-based metrics, it would be interesting to compare the two: given a set of identical drugs, how does behave the similarity measures? Are the data inline? What differences are observed? For which set of compounds? How does the definition of function differ from a theoretical perspective (thesis) against the one derived from experimental values (CMap)? The FTC also provides discrete categories for MoAs (toolbox approach), it could be interesting to investigate what advantage it brings when compared to gene expression data.

In this regard, I have already started to gather the data used in the work done by (//cite francesco). Once available, one can start the investigation and analysis of the definition of approved drug’s function.

\section{Biomedical knowledge: towards a simpler representation}
From a computer science perspective, ontologies are computational artefacts used to represent the knowledge of a domain of interest. The motivation behind the approach is to clarify the semantics of a vocabulary, in order to validate the consistency of a representation and derive implicit information (//ref chapter 2). The axioms helps computer to unambiguously interpret some knowledge. However in practice in the life domain, human interpretation is always required. For instance the content of the Gene Ontology is mostly used to perform term enrichment analysis (//cite and personal experience) or similarity based computations (//cite), in order to guide the interpretation of some generated data. The DLs based representation of the biomedical knowledge presented in Chapter 2 can be arguably seen as heavy and too complicated for the results sought. The axioms present in the FTC are based on existential restriction patterns in order to logically link concepts via an object property. This representation could be simplified by considering all concepts as instances directly linked by a property (role assertion //ref chapter 2) and resulting in a graph. This model is close to the one proposed by the Resource Description Framework (RDF), one of the semantic web’s specification (//cite RDF). The ambiguity between instances and concepts in known as punning and allowed by the Web Ontology Language; it is widely used in the RDF representation of biomedical databases such as ChEMBL or Uniprot (//cite and URL to end-point), as it allows to straightforwardly represent the required information.

Considering this, the future work I will do on biomedical knowledge representation will go in this direction: representing life science knowledge as a simple graph while keeping the expressivity of roles. I and others expressed most of the FTC semantics using roles, highly interoperable one with another (//ref Chapter 3 specs FTC - regulates, part-of, etc… and GO website). As far as I know, there exists no framework within the semantic web specification stack (RDF, RDFS, OWL and related profiles) allowing the extended use of object property or role axioms (transitivity, chained property) in combination with a simple graph representation as the one featured by RDF. With such a language, it would be possible to keep the advantages of scalable reasoning (subset of EL++) while considerably simplifying the initial representation and discarding cumbersome axioms not so helpful for biological interpretation later on, for instance existential restrictions.

The language will lie between network biology (//cite), semantic nets (//cite), Petri Nets (//cite), pathways databases (//cite nature pathways, reactome, kegg) and RDF while keeping the important reasoning services. The added value on the top of these previous approaches will reside in the importance given to the edges between nodes, in particular their biomedical semantics and what can be derived and expanded from it. The black box machine analogy is still valid for this representation, the implementation can be done in many other ways than the one presented with the FTC. The focus of the language would be the deductions that can be derived from it rather than maximizing interoperability between resources. A graph representation also allows to fully control data query and analysis; it would also be easier to derive metrics such as similarity values, commonly used in biology (//cite robert stevens OWL). Moreover, some robust solution are available at the time of writing (//cite graph paper for databases).

More work has to be done in order to fully express the idea, but it can be summarised as a simple and customisable graph-based framework to locally integrate biomedical information, with a strong focus on edges semantics interoperability (part-of, regulates, involved-in combined with rules).

\section{Off-label uses and clinical drug repositioning}
In the context of my work, I came to discuss drug repositioning with some physicians practicing in hospital (Addenbrooke's - Cambridge UK). After the meetings and analyses presented on Chapter 4, it appeared clearly that drug repositioning relates a lot to off-label prescriptions. In practice, doctors twenty-percent of the time indicate a treatment differently to what it is regulated for (//cite off-label papers). Sometimes a new pharmacological effect is identified (//ref thalidomide), and the indication line of the drug can be expanded accordingly. This type of evidence is very valuable, as it reflects the effect of drug directly in human population. A meticulous recording and automated analysis can help to identify such cases rapidly. Off-label prescription data is currently not recorded in all hospitals (personal discussion with physicians), and a computer system handling it might be a smart way to directly capture relevant hypotheses. It could be developed on the top of an electronic medical records system (//cite paper off-labels) or as a stand alone application. I am currently investigating what would be the requirements of such a workflow, in collaboration with physicians and biomedical researchers.

\hrulefill

This chapter concludes the thesis work I decided to present in more details. The reader can refer to the Annexes in order to find the full list of the peer-reviewed articles I contributed to and published in the course of the doctorate, as a complementary source of information. I hope to have presented clearly and concisely the message I wanted to communicate: Living organisms, just as machines, are subject to dysfunction which can be investigated for repair using description logics. The source of this document is free, open and available online at https://github.com/loopasam/thesis.

